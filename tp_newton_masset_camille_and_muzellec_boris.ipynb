{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# LAB: (quasi-) Newton methods\n",
    "\n",
    "Author : Alexandre Gramfort\n",
    "\n",
    "The objective of this lab session is to implement:\n",
    "- Newton method\n",
    "- DFP\n",
    "- BFGS\n",
    "and compare your implementation with the BFGS and L-BFGS solvers in scipy\n",
    "\n",
    "You will need to knowledge on **line search methods** and **conjugate gradient** in particular you should reuse the notebooks presented during the lectures.\n",
    "\n",
    "## VERY IMPORTANT\n",
    "\n",
    "- This work **must be done by pairs of students**.\n",
    "- **Each** student must send their work **before the 30th of october at 23:59**, using the **moodle platform**.\n",
    "- This means that **each student in the pair sends the same file**\n",
    "- On the moodle, in the \"Optimization for Data Science\" course, you have a \"devoir\" section called **Rendu TP du 24 octobre 2016**. This is where you submit your jupyter notebook file. \n",
    "- The **name of the file must be** constructed as in the next cell\n",
    "\n",
    "# Gentle reminder: no evaluation if you don't respect this EXACTLY\n",
    "\n",
    "### How to construct the name of your file"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# Change here using YOUR first and last names\n",
    "fn1 = \"camille\"\n",
    "ln1 = \"masset\"\n",
    "fn2 = \"boris\"\n",
    "ln2 = \"muzellec\"\n",
    "\n",
    "filename = \"_\".join(map(lambda s: s.strip().lower(), \n",
    "                        [\"tp_newton\", ln1, fn1, \"and\", ln2, fn2])) + \".ipynb\"\n",
    "print(filename)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Part 0: Demo using Gradient descent\n",
    "\n",
    "First import the necessary libraries:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "from scipy import optimize\n",
    "\n",
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now import the necessary function from the optim_utils.py file."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "from optim_utils import test_solver"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "You'll have work only with the `test_solver` function.\n",
    "\n",
    "This function expects a function as parameter.\n",
    "\n",
    "The signature of the function `optimizer` to pass should be the following:\n",
    "\n",
    "`optimizer(x0, f, f_grad, f_hessian)`\n",
    "\n",
    "Let us now make an example with a gradient descent."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def gradient_descent(x0, f, f_grad, f_hessian=None):\n",
    "\n",
    "    default_step = 0.01\n",
    "    c1 = 0.0001\n",
    "    c2 = 0.9\n",
    "    max_iter = 100\n",
    "    \n",
    "    # This variable is used to indicate whether or not we want to print\n",
    "    # monitoring information (iteration counter, function value and norm of the gradient)\n",
    "    verbose = True\n",
    "\n",
    "    all_x_k, all_f_k = list(), list()\n",
    "    x = x0\n",
    "\n",
    "    all_x_k.append(x.copy())\n",
    "    all_f_k.append(f(x))\n",
    "\n",
    "    for k in range(1, max_iter + 1):\n",
    "\n",
    "        grad_x = f_grad(x)\n",
    "\n",
    "        # Compute a step size using a line_search to satisfy the\n",
    "        # strong Wolfe conditions\n",
    "        step, _, _, new_f, _, new_grad = optimize.line_search(f, f_grad, x,\n",
    "                                                              -grad_x, grad_x,\n",
    "                                                              c1=c1, c2=c2)\n",
    "        if step is None:\n",
    "            print(\"Line search did not converge at iteration %s\" % k)\n",
    "            step = default_step\n",
    "\n",
    "        x -= step * grad_x\n",
    "\n",
    "        all_x_k.append(x.copy())\n",
    "        all_f_k.append(new_f)\n",
    "\n",
    "        l_inf_norm_grad = np.max(np.abs(new_grad))\n",
    "\n",
    "        if verbose:\n",
    "            print('iter: %d, f: %.6g, l_inf_norm(grad): %.6g' %\n",
    "                  (k, new_f, l_inf_norm_grad))\n",
    "\n",
    "        if l_inf_norm_grad < 1e-6:\n",
    "            break\n",
    "\n",
    "    return np.array(all_x_k), np.array(all_f_k)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now I can call the `test_solver` function with this solver:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "test_solver(gradient_descent)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Part 2: Implement Newton method\n",
    "\n",
    "You now need to implement Newton method using either `linalg.solve` or `sparse.linalg.cg` solve the linear system at each iteration. You need to implement both versions.\n",
    "\n",
    "You're expected to comment in a few lines what you see."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import scipy.linalg as linalg\n",
    "import scipy.sparse as sparse\n",
    "\n",
    "def newton(x0, f, f_grad, f_hessian):\n",
    "    default_step = 0.01\n",
    "    c1 = 0.0001\n",
    "    c2 = 0.9\n",
    "    max_iter = 100\n",
    "    \n",
    "    # This variable is used to indicate whether or not we want to print\n",
    "    # monitoring information (iteration counter, function value and norm of the gradient)\n",
    "    verbose = True\n",
    "\n",
    "    all_x_k, all_f_k = list(), list()\n",
    "    x = x0\n",
    "\n",
    "    all_x_k.append(x.copy())\n",
    "    all_f_k.append(f(x))\n",
    "\n",
    "    for k in range(1, max_iter + 1):\n",
    "\n",
    "        grad_x = f_grad(x)\n",
    "        \n",
    "        ####################################\n",
    "        # Compute here the search direction\n",
    "        # d = ...\n",
    "        ####################################\n",
    "        \n",
    "        d = linalg.solve(f_hessian(x), -grad_x)\n",
    "        #d, _ = sparse.linalg.cg(f_hessian(x), -grad_x)\n",
    "\n",
    "        # Compute a step size using a line_search to satisfy the\n",
    "        # strong Wolfe conditions\n",
    "        step, _, _, new_f, _, new_grad = optimize.line_search(f, f_grad, x,\n",
    "                                                              d, grad_x,\n",
    "                                                              c1=c1, c2=c2)\n",
    "        if step is None:\n",
    "            print(\"Line search did not converge at iteration %s\" % k)\n",
    "            step = default_step\n",
    "\n",
    "        ##################################\n",
    "        # Compute here the new value of x\n",
    "        # x = ...\n",
    "        ##################################\n",
    "        \n",
    "        x += step * d\n",
    "\n",
    "        all_x_k.append(x.copy())\n",
    "        all_f_k.append(new_f)\n",
    "\n",
    "        l_inf_norm_grad = np.max(np.abs(new_grad))\n",
    "\n",
    "        if verbose:\n",
    "            print('iter: %d, f: %.6g, l_inf_norm(grad): %.6g' %\n",
    "                  (k, new_f, l_inf_norm_grad))\n",
    "\n",
    "        if l_inf_norm_grad < 1e-6:\n",
    "            break\n",
    "\n",
    "    return np.array(all_x_k), np.array(all_f_k)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "test_solver(newton)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Your comments here\n",
    "\n",
    "We can compare the Newton method with Gradient Descent.  \n",
    "\n",
    "First, we notice that in the quadratic case (case #1), the Newton method converges in 1 iteration, and the resulting $x_{\\text{min}}$ is exact (error = 0), as we saw during the lecture, whereas with the Gradient Descent, it took 33 iterations and the result was approximate.  \n",
    "For the second case, Newton converges also in 1 iteration, whereas with Gradient Descent, we did not reach the wanted precision within the maximal number of iterations. When we use the solver provided by `scipy.linalg.solve`, the solution is exact, but with the Conjugate Gradient algorithm, it is approximate.  \n",
    "For the third case, Newton converges in 12 iterations, and with a far better precision than Gradient Descent. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Part 2: Implement DFP algorithm\n",
    "\n",
    "You now need to implement the DFP algorithm."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def dfp(x0, f, f_grad, f_hessian):\n",
    "    default_step = 0.01\n",
    "    c1 = 0.0001\n",
    "    c2 = 0.95\n",
    "    max_iter = 200\n",
    "    \n",
    "    # This variable is used to indicate whether or not we want to print\n",
    "    # monitoring information (iteration counter, function value and norm of the gradient)\n",
    "    verbose = True\n",
    "\n",
    "    all_x_k, all_f_k = list(), list()\n",
    "    x = x0\n",
    "\n",
    "    all_x_k.append(x.copy())\n",
    "    all_f_k.append(f(x))\n",
    "\n",
    "    B = np.eye(len(x))  # inverse Hessian approximation<\n",
    "    \n",
    "    for k in range(1, max_iter + 1):       \n",
    "        \n",
    "        ####################################\n",
    "        # Compute here the search direction\n",
    "        # d = ...\n",
    "        ####################################\n",
    "        \n",
    "        grad_x = f_grad(x)\n",
    "\n",
    "        d = -B.dot(grad_x)\n",
    "\n",
    "        # Compute a step size using a line_search to satisfy the\n",
    "        # strong Wolfe conditions\n",
    "        step, _, _, new_f, _, new_grad_x = optimize.line_search(f, f_grad, x,\n",
    "                                                              d, grad_x,\n",
    "                                                              c1=c1, c2=c2)\n",
    "        \n",
    "        if step is None:\n",
    "            print(\"Line search did not converge at iteration %s\" % k)\n",
    "            step = default_step\n",
    "\n",
    "        ##################################\n",
    "        # Compute here the new value of x\n",
    "        # x = ...\n",
    "        ##################################\n",
    "        \n",
    "        s = step*d\n",
    "        x += s\n",
    "        \n",
    "        #########################################################\n",
    "        # Update the inverse Hessian approximation\n",
    "        # B = ...\n",
    "        #########################################################\n",
    "        \n",
    "        y = new_grad_x - grad_x\n",
    "        \n",
    "        By = B.dot(y)\n",
    "        \n",
    "        B += np.matrix(s).T.dot(np.matrix(s))/s.dot(y) - np.matrix(By).T.dot(np.matrix(By))/(y.dot(By))\n",
    "        \n",
    "        all_x_k.append(x.copy())\n",
    "        all_f_k.append(new_f)\n",
    "\n",
    "        l_inf_norm_grad = np.max(np.abs(new_grad_x))\n",
    "\n",
    "        if verbose:\n",
    "            print('iter: %d, f: %.6g, l_inf_norm(grad): %.6g' %\n",
    "                  (k, new_f, l_inf_norm_grad))\n",
    "\n",
    "        if l_inf_norm_grad < 1e-6:\n",
    "            break\n",
    "            \n",
    "    return np.array(all_x_k), np.array(all_f_k)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "test_solver(dfp)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Your comments here"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Part 3: Implement BFGS algorithm\n",
    "\n",
    "You now need to implement the BFGS algorithm."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def bfgs(x0, f, f_grad, f_hessian):\n",
    "    default_step = 0.01\n",
    "    c1 = 0.0001\n",
    "    c2 = 0.9\n",
    "    max_iter = 100\n",
    "    \n",
    "    # This variable is used to indicate whether or not we want to print\n",
    "    # monitoring information (iteration counter, function value and norm of the gradient)\n",
    "    verbose = True\n",
    "\n",
    "    all_x_k, all_f_k = list(), list()\n",
    "    x = x0\n",
    "\n",
    "    all_x_k.append(x.copy())\n",
    "    all_f_k.append(f(x))\n",
    "\n",
    "    H = np.eye(len(x))  # Hessian approximation\n",
    "    \n",
    "    grad_x = f_grad(x)\n",
    "    \n",
    "    for k in range(1, max_iter + 1):       \n",
    "        \n",
    "        ####################################\n",
    "        # Compute here the search direction\n",
    "        # d = ...\n",
    "        ####################################\n",
    "        \n",
    "        d = linalg.solve(H, -grad_x)\n",
    "        \n",
    "        # Compute a step size using a line_search to satisfy the\n",
    "        # strong Wolfe conditions\n",
    "        step, _, _, new_f, _, new_grad = optimize.line_search(f, f_grad, x,\n",
    "                                                              d, grad_x,\n",
    "                                                              c1=c1, c2=c2)\n",
    "                \n",
    "        if step is None:\n",
    "            print(\"Line search did not converge at iteration %s\" % k)\n",
    "            step = default_step\n",
    "\n",
    "        ##################################    \n",
    "        # Compute here the new value of x\n",
    "        # x = ...\n",
    "        ##################################\n",
    "        \n",
    "        s = step*d\n",
    "        x += s\n",
    "        \n",
    "        #####################################################\n",
    "        # Update the Hessian approximation   \n",
    "        # H = ...\n",
    "        #####################################################\n",
    "        \n",
    "        y = new_grad - grad_x\n",
    "        \n",
    "        Hs = H.dot(s)\n",
    "        \n",
    "        H += np.matrix(y).T.dot(np.matrix(y))/y.dot(s) - np.matrix(Hs).T.dot(np.matrix(Hs))/(s.dot(Hs))\n",
    "        \n",
    "        all_x_k.append(x.copy())\n",
    "        all_f_k.append(new_f)\n",
    "\n",
    "        l_inf_norm_grad = np.max(np.abs(new_grad))\n",
    "\n",
    "        if verbose:\n",
    "            print('iter: %d, f: %.6g, l_inf_norm(grad): %.6g' %\n",
    "                  (k, new_f, l_inf_norm_grad))\n",
    "\n",
    "        if l_inf_norm_grad < 1e-6:\n",
    "            break\n",
    "            \n",
    "        grad_x = new_grad\n",
    "\n",
    "    return np.array(all_x_k), np.array(all_f_k)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "test_solver(bfgs)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Your comments here\n",
    "\n",
    "Much less iterations than dfp (except for the quadratic case)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Part 4: Do the same now with scipy implementation of BFGS and L-BFGS"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from scipy.optimize import fmin_bfgs, fmin_l_bfgs_b\n",
    "\n",
    "# TODO"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Your comments here"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.2"
  },
  "widgets": {
   "state": {},
   "version": "1.1.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
